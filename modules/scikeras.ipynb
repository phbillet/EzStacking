{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3becd89-a19e-44a4-ab94-4298fd0d143b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d57617-17c1-4f2b-8c0b-e86aa982217d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "814ac270-f39b-4878-b7ed-dcce4ef7f96b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "N = 123\n",
    "T = (N,)\n",
    "print(type(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ffdb1ad-9a42-4cf3-84de-0e7b165427b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Any\n",
    "\n",
    "class MLPClassifier(KerasClassifier):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=100,\n",
    "        activation=\"relu\",\n",
    "        batch_normalization=True,\n",
    "        dropout=0.0,\n",
    "        optimizer=\"adam\",\n",
    "        optimizer__learning_rate=0.001,\n",
    "        epochs=200,\n",
    "        verbose=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout = dropout\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs: Dict[str, Any]):\n",
    "        model = keras.Sequential()\n",
    "        inp = keras.layers.Input(shape=(self.n_features_in_))\n",
    "        model.add(inp)\n",
    "        for hidden_layer_size in (self.hidden_layer_sizes,):\n",
    "            layer = keras.layers.Dense(hidden_layer_size, activation=self.activation)\n",
    "            model.add(layer)\n",
    "            if self.batch_normalization:\n",
    "               layer = keras.layers.BatchNormalization()\n",
    "               model.add(layer)\n",
    "            if self.dropout > 0.0:\n",
    "               layer = keras.layers.Dropout(self.dropout)\n",
    "               model.add(layer)    \n",
    "        if self.target_type_ == \"binary\":\n",
    "            n_output_units = 1\n",
    "            output_activation = \"sigmoid\"\n",
    "            loss = \"binary_crossentropy\"\n",
    "        elif self.target_type_ == \"multiclass\":\n",
    "            n_output_units = self.n_classes_\n",
    "            output_activation = \"softmax\"\n",
    "            loss = \"sparse_categorical_crossentropy\"\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported task type: {self.target_type_}\")\n",
    "        out = keras.layers.Dense(n_output_units, activation=output_activation)\n",
    "        model.add(out)\n",
    "        model.compile(loss=loss, optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69e5bd84-6c00-4471-b3fe-1d2cc05b7647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "15/15 [==============================] - 1s 7ms/step - loss: 0.8322 - val_loss: 0.9059 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.7749 - val_loss: 0.7729 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7456 - val_loss: 0.6945 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6887 - val_loss: 0.6421 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6597 - val_loss: 0.6041 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6655 - val_loss: 0.5771 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6472 - val_loss: 0.5539 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.5439 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 0.5353 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6077 - val_loss: 0.5212 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6065 - val_loss: 0.5126 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.5049 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.4961 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 0.4964 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5711 - val_loss: 0.4892 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.4792 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5836 - val_loss: 0.4741 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5950 - val_loss: 0.4677 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 0.4643 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5579 - val_loss: 0.4587 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5666 - val_loss: 0.4540 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5569 - val_loss: 0.4498 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5704 - val_loss: 0.4434 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5494 - val_loss: 0.4414 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5463 - val_loss: 0.4395 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5537 - val_loss: 0.4420 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5327 - val_loss: 0.4338 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.4320 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.4289 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5433 - val_loss: 0.4297 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.4254 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.4217 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5409 - val_loss: 0.4187 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.4148 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5225 - val_loss: 0.4162 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.4182 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5192 - val_loss: 0.4205 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.4205 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5201 - val_loss: 0.4205 - lr: 0.0010\n",
      "Epoch 40/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 0.4164 - lr: 0.0010\n",
      "Epoch 41/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5136 - val_loss: 0.4116 - lr: 0.0010\n",
      "Epoch 42/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5219 - val_loss: 0.4120 - lr: 0.0010\n",
      "Epoch 43/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5141 - val_loss: 0.4120 - lr: 0.0010\n",
      "Epoch 44/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5088 - val_loss: 0.4121 - lr: 0.0010\n",
      "Epoch 45/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5067 - val_loss: 0.4100 - lr: 0.0010\n",
      "Epoch 46/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5011 - val_loss: 0.4082 - lr: 0.0010\n",
      "Epoch 47/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5135 - val_loss: 0.4071 - lr: 0.0010\n",
      "Epoch 48/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5008 - val_loss: 0.4088 - lr: 0.0010\n",
      "Epoch 49/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5163 - val_loss: 0.4091 - lr: 0.0010\n",
      "Epoch 50/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4984 - val_loss: 0.4060 - lr: 0.0010\n",
      "Epoch 51/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4948 - val_loss: 0.4061 - lr: 0.0010\n",
      "Epoch 52/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5095 - val_loss: 0.4068 - lr: 0.0010\n",
      "Epoch 53/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4992 - val_loss: 0.4062 - lr: 0.0010\n",
      "Epoch 54/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5034 - val_loss: 0.4006 - lr: 0.0010\n",
      "Epoch 55/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4999 - val_loss: 0.3971 - lr: 0.0010\n",
      "Epoch 56/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4997 - val_loss: 0.3955 - lr: 0.0010\n",
      "Epoch 57/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5095 - val_loss: 0.3977 - lr: 0.0010\n",
      "Epoch 58/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4810 - val_loss: 0.3986 - lr: 0.0010\n",
      "Epoch 59/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4968 - val_loss: 0.3951 - lr: 0.0010\n",
      "Epoch 60/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4952 - val_loss: 0.3937 - lr: 0.0010\n",
      "Epoch 61/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4847 - val_loss: 0.3914 - lr: 0.0010\n",
      "Epoch 62/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4825 - val_loss: 0.3883 - lr: 0.0010\n",
      "Epoch 63/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4914 - val_loss: 0.3862 - lr: 0.0010\n",
      "Epoch 64/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4858 - val_loss: 0.3850 - lr: 0.0010\n",
      "Epoch 65/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4905 - val_loss: 0.3858 - lr: 0.0010\n",
      "Epoch 66/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4973 - val_loss: 0.3862 - lr: 0.0010\n",
      "Epoch 67/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4839 - val_loss: 0.3889 - lr: 0.0010\n",
      "Epoch 68/2000\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4944 - val_loss: 0.3886 - lr: 0.0010\n",
      "Epoch 69/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4835 - val_loss: 0.3891 - lr: 0.0010\n",
      "Epoch 70/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4778 - val_loss: 0.3904 - lr: 0.0010\n",
      "Epoch 71/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4769 - val_loss: 0.3895 - lr: 0.0010\n",
      "Epoch 72/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4844 - val_loss: 0.3877 - lr: 0.0010\n",
      "Epoch 73/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4765 - val_loss: 0.3855 - lr: 0.0010\n",
      "Epoch 74/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4591 - val_loss: 0.3852 - lr: 0.0010\n",
      "Epoch 75/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4784 - val_loss: 0.3823 - lr: 0.0010\n",
      "Epoch 76/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4611 - val_loss: 0.3799 - lr: 0.0010\n",
      "Epoch 77/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4861 - val_loss: 0.3802 - lr: 0.0010\n",
      "Epoch 78/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4661 - val_loss: 0.3820 - lr: 0.0010\n",
      "Epoch 79/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4670 - val_loss: 0.3799 - lr: 0.0010\n",
      "Epoch 80/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4731 - val_loss: 0.3757 - lr: 0.0010\n",
      "Epoch 81/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4803 - val_loss: 0.3706 - lr: 0.0010\n",
      "Epoch 82/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4540 - val_loss: 0.3687 - lr: 0.0010\n",
      "Epoch 83/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4816 - val_loss: 0.3692 - lr: 0.0010\n",
      "Epoch 84/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4540 - val_loss: 0.3652 - lr: 0.0010\n",
      "Epoch 85/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4711 - val_loss: 0.3658 - lr: 0.0010\n",
      "Epoch 86/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4577 - val_loss: 0.3668 - lr: 0.0010\n",
      "Epoch 87/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4677 - val_loss: 0.3651 - lr: 0.0010\n",
      "Epoch 88/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4832 - val_loss: 0.3635 - lr: 0.0010\n",
      "Epoch 89/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4652 - val_loss: 0.3622 - lr: 0.0010\n",
      "Epoch 90/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4529 - val_loss: 0.3601 - lr: 0.0010\n",
      "Epoch 91/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4610 - val_loss: 0.3579 - lr: 0.0010\n",
      "Epoch 92/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4634 - val_loss: 0.3584 - lr: 0.0010\n",
      "Epoch 93/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4676 - val_loss: 0.3589 - lr: 0.0010\n",
      "Epoch 94/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4542 - val_loss: 0.3604 - lr: 0.0010\n",
      "Epoch 95/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4692 - val_loss: 0.3589 - lr: 0.0010\n",
      "Epoch 96/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4610 - val_loss: 0.3613 - lr: 0.0010\n",
      "Epoch 97/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4635 - val_loss: 0.3631 - lr: 0.0010\n",
      "Epoch 98/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4594 - val_loss: 0.3647 - lr: 0.0010\n",
      "Epoch 99/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4344 - val_loss: 0.3657 - lr: 0.0010\n",
      "Epoch 100/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4585 - val_loss: 0.3654 - lr: 0.0010\n",
      "Epoch 101/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4479 - val_loss: 0.3619 - lr: 0.0010\n",
      "Epoch 102/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4478 - val_loss: 0.3603 - lr: 0.0010\n",
      "Epoch 103/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4644 - val_loss: 0.3570 - lr: 0.0010\n",
      "Epoch 104/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4580 - val_loss: 0.3566 - lr: 0.0010\n",
      "Epoch 105/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4566 - val_loss: 0.3558 - lr: 0.0010\n",
      "Epoch 106/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4524 - val_loss: 0.3544 - lr: 0.0010\n",
      "Epoch 107/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4452 - val_loss: 0.3530 - lr: 0.0010\n",
      "Epoch 108/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4320 - val_loss: 0.3545 - lr: 0.0010\n",
      "Epoch 109/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4416 - val_loss: 0.3562 - lr: 0.0010\n",
      "Epoch 110/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4416 - val_loss: 0.3575 - lr: 0.0010\n",
      "Epoch 111/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4520 - val_loss: 0.3554 - lr: 0.0010\n",
      "Epoch 112/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4591 - val_loss: 0.3553 - lr: 0.0010\n",
      "Epoch 113/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4322 - val_loss: 0.3532 - lr: 0.0010\n",
      "Epoch 114/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4374 - val_loss: 0.3512 - lr: 0.0010\n",
      "Epoch 115/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4571 - val_loss: 0.3514 - lr: 0.0010\n",
      "Epoch 116/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4400 - val_loss: 0.3480 - lr: 0.0010\n",
      "Epoch 117/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4300 - val_loss: 0.3470 - lr: 0.0010\n",
      "Epoch 118/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4451 - val_loss: 0.3451 - lr: 0.0010\n",
      "Epoch 119/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4524 - val_loss: 0.3471 - lr: 0.0010\n",
      "Epoch 120/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4457 - val_loss: 0.3478 - lr: 0.0010\n",
      "Epoch 121/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4376 - val_loss: 0.3554 - lr: 0.0010\n",
      "Epoch 122/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4444 - val_loss: 0.3559 - lr: 0.0010\n",
      "Epoch 123/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4471 - val_loss: 0.3575 - lr: 0.0010\n",
      "Epoch 124/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4476 - val_loss: 0.3546 - lr: 0.0010\n",
      "Epoch 125/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4548 - val_loss: 0.3527 - lr: 0.0010\n",
      "Epoch 126/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4473 - val_loss: 0.3499 - lr: 0.0010\n",
      "Epoch 127/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4492 - val_loss: 0.3476 - lr: 0.0010\n",
      "Epoch 128/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4326 - val_loss: 0.3426 - lr: 0.0010\n",
      "Epoch 129/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4477 - val_loss: 0.3412 - lr: 0.0010\n",
      "Epoch 130/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4408 - val_loss: 0.3494 - lr: 0.0010\n",
      "Epoch 131/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4376 - val_loss: 0.3491 - lr: 0.0010\n",
      "Epoch 132/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4330 - val_loss: 0.3484 - lr: 0.0010\n",
      "Epoch 133/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4366 - val_loss: 0.3478 - lr: 0.0010\n",
      "Epoch 134/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4199 - val_loss: 0.3485 - lr: 0.0010\n",
      "Epoch 135/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4248 - val_loss: 0.3464 - lr: 0.0010\n",
      "Epoch 136/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4364 - val_loss: 0.3440 - lr: 0.0010\n",
      "Epoch 137/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4106 - val_loss: 0.3443 - lr: 0.0010\n",
      "Epoch 138/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4294 - val_loss: 0.3461 - lr: 0.0010\n",
      "Epoch 139/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4176 - val_loss: 0.3481 - lr: 0.0010\n",
      "Epoch 140/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4311 - val_loss: 0.3488 - lr: 0.0010\n",
      "Epoch 141/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4349 - val_loss: 0.3482 - lr: 0.0010\n",
      "Epoch 142/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4443 - val_loss: 0.3442 - lr: 0.0010\n",
      "Epoch 143/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4242 - val_loss: 0.3433 - lr: 0.0010\n",
      "Epoch 144/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4389 - val_loss: 0.3390 - lr: 0.0010\n",
      "Epoch 145/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4151 - val_loss: 0.3382 - lr: 0.0010\n",
      "Epoch 146/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4368 - val_loss: 0.3385 - lr: 0.0010\n",
      "Epoch 147/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4227 - val_loss: 0.3412 - lr: 0.0010\n",
      "Epoch 148/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4167 - val_loss: 0.3443 - lr: 0.0010\n",
      "Epoch 149/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4232 - val_loss: 0.3418 - lr: 0.0010\n",
      "Epoch 150/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4141 - val_loss: 0.3386 - lr: 0.0010\n",
      "Epoch 151/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4231 - val_loss: 0.3365 - lr: 0.0010\n",
      "Epoch 152/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4422 - val_loss: 0.3350 - lr: 0.0010\n",
      "Epoch 153/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4041 - val_loss: 0.3328 - lr: 0.0010\n",
      "Epoch 154/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4120 - val_loss: 0.3293 - lr: 0.0010\n",
      "Epoch 155/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4166 - val_loss: 0.3304 - lr: 0.0010\n",
      "Epoch 156/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4214 - val_loss: 0.3321 - lr: 0.0010\n",
      "Epoch 157/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4103 - val_loss: 0.3325 - lr: 0.0010\n",
      "Epoch 158/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4132 - val_loss: 0.3335 - lr: 0.0010\n",
      "Epoch 159/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4074 - val_loss: 0.3295 - lr: 0.0010\n",
      "Epoch 160/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4222 - val_loss: 0.3278 - lr: 0.0010\n",
      "Epoch 161/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4074 - val_loss: 0.3267 - lr: 0.0010\n",
      "Epoch 162/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4161 - val_loss: 0.3270 - lr: 0.0010\n",
      "Epoch 163/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4224 - val_loss: 0.3257 - lr: 0.0010\n",
      "Epoch 164/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4115 - val_loss: 0.3246 - lr: 0.0010\n",
      "Epoch 165/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4125 - val_loss: 0.3234 - lr: 0.0010\n",
      "Epoch 166/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4046 - val_loss: 0.3217 - lr: 0.0010\n",
      "Epoch 167/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4170 - val_loss: 0.3212 - lr: 0.0010\n",
      "Epoch 168/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4116 - val_loss: 0.3204 - lr: 0.0010\n",
      "Epoch 169/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4121 - val_loss: 0.3172 - lr: 0.0010\n",
      "Epoch 170/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4018 - val_loss: 0.3159 - lr: 0.0010\n",
      "Epoch 171/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4093 - val_loss: 0.3144 - lr: 0.0010\n",
      "Epoch 172/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4323 - val_loss: 0.3147 - lr: 0.0010\n",
      "Epoch 173/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4022 - val_loss: 0.3168 - lr: 0.0010\n",
      "Epoch 174/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4127 - val_loss: 0.3146 - lr: 0.0010\n",
      "Epoch 175/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4106 - val_loss: 0.3134 - lr: 0.0010\n",
      "Epoch 176/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4098 - val_loss: 0.3133 - lr: 0.0010\n",
      "Epoch 177/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4078 - val_loss: 0.3106 - lr: 0.0010\n",
      "Epoch 178/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4020 - val_loss: 0.3090 - lr: 0.0010\n",
      "Epoch 179/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3935 - val_loss: 0.3062 - lr: 0.0010\n",
      "Epoch 180/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4132 - val_loss: 0.3056 - lr: 0.0010\n",
      "Epoch 181/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4086 - val_loss: 0.3071 - lr: 0.0010\n",
      "Epoch 182/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3981 - val_loss: 0.3050 - lr: 0.0010\n",
      "Epoch 183/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.3051 - lr: 0.0010\n",
      "Epoch 184/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4123 - val_loss: 0.3071 - lr: 0.0010\n",
      "Epoch 185/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3981 - val_loss: 0.3076 - lr: 0.0010\n",
      "Epoch 186/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4014 - val_loss: 0.3098 - lr: 0.0010\n",
      "Epoch 187/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4006 - val_loss: 0.3089 - lr: 0.0010\n",
      "Epoch 188/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3900 - val_loss: 0.3069 - lr: 0.0010\n",
      "Epoch 189/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3941 - val_loss: 0.3037 - lr: 0.0010\n",
      "Epoch 190/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3768 - val_loss: 0.3032 - lr: 0.0010\n",
      "Epoch 191/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4020 - val_loss: 0.3020 - lr: 0.0010\n",
      "Epoch 192/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3934 - val_loss: 0.3017 - lr: 0.0010\n",
      "Epoch 193/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4002 - val_loss: 0.3011 - lr: 0.0010\n",
      "Epoch 194/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3874 - val_loss: 0.3040 - lr: 0.0010\n",
      "Epoch 195/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.3050 - lr: 0.0010\n",
      "Epoch 196/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3884 - val_loss: 0.3051 - lr: 0.0010\n",
      "Epoch 197/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3933 - val_loss: 0.3036 - lr: 0.0010\n",
      "Epoch 198/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4006 - val_loss: 0.3012 - lr: 0.0010\n",
      "Epoch 199/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3886 - val_loss: 0.2988 - lr: 0.0010\n",
      "Epoch 200/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.2979 - lr: 0.0010\n",
      "Epoch 201/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4002 - val_loss: 0.2971 - lr: 0.0010\n",
      "Epoch 202/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3738 - val_loss: 0.2952 - lr: 0.0010\n",
      "Epoch 203/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3822 - val_loss: 0.2943 - lr: 0.0010\n",
      "Epoch 204/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3903 - val_loss: 0.2921 - lr: 0.0010\n",
      "Epoch 205/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3938 - val_loss: 0.2920 - lr: 0.0010\n",
      "Epoch 206/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3930 - val_loss: 0.2927 - lr: 0.0010\n",
      "Epoch 207/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3953 - val_loss: 0.2915 - lr: 0.0010\n",
      "Epoch 208/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3871 - val_loss: 0.2897 - lr: 0.0010\n",
      "Epoch 209/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3829 - val_loss: 0.2895 - lr: 0.0010\n",
      "Epoch 210/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3936 - val_loss: 0.2895 - lr: 0.0010\n",
      "Epoch 211/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3863 - val_loss: 0.2888 - lr: 0.0010\n",
      "Epoch 212/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3904 - val_loss: 0.2901 - lr: 0.0010\n",
      "Epoch 213/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3907 - val_loss: 0.2952 - lr: 0.0010\n",
      "Epoch 214/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3991 - val_loss: 0.2971 - lr: 0.0010\n",
      "Epoch 215/2000\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4104 - val_loss: 0.2956 - lr: 0.0010\n",
      "Epoch 216/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3889 - val_loss: 0.2945 - lr: 0.0010\n",
      "Epoch 217/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3847 - val_loss: 0.2932 - lr: 0.0010\n",
      "Epoch 218/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.2921 - lr: 0.0010\n",
      "Epoch 219/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3869 - val_loss: 0.2899 - lr: 0.0010\n",
      "Epoch 220/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3867 - val_loss: 0.2870 - lr: 0.0010\n",
      "Epoch 221/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3781 - val_loss: 0.2838 - lr: 0.0010\n",
      "Epoch 222/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3833 - val_loss: 0.2812 - lr: 0.0010\n",
      "Epoch 223/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3635 - val_loss: 0.2789 - lr: 0.0010\n",
      "Epoch 224/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3644 - val_loss: 0.2777 - lr: 0.0010\n",
      "Epoch 225/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4014 - val_loss: 0.2779 - lr: 0.0010\n",
      "Epoch 226/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.2778 - lr: 0.0010\n",
      "Epoch 227/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 0.2764 - lr: 0.0010\n",
      "Epoch 228/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3940 - val_loss: 0.2748 - lr: 0.0010\n",
      "Epoch 229/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3802 - val_loss: 0.2758 - lr: 0.0010\n",
      "Epoch 230/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3740 - val_loss: 0.2752 - lr: 0.0010\n",
      "Epoch 231/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.2759 - lr: 0.0010\n",
      "Epoch 232/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3637 - val_loss: 0.2786 - lr: 0.0010\n",
      "Epoch 233/2000\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3433 - val_loss: 0.2800 - lr: 0.0010\n",
      "Epoch 234/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3632 - val_loss: 0.2806 - lr: 0.0010\n",
      "Epoch 235/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.2801 - lr: 0.0010\n",
      "Epoch 236/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3593 - val_loss: 0.2793 - lr: 0.0010\n",
      "Epoch 237/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 0.2781 - lr: 0.0010\n",
      "Epoch 238/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3687 - val_loss: 0.2747 - lr: 0.0010\n",
      "Epoch 239/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3574 - val_loss: 0.2729 - lr: 0.0010\n",
      "Epoch 240/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3567 - val_loss: 0.2699 - lr: 0.0010\n",
      "Epoch 241/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3434 - val_loss: 0.2679 - lr: 0.0010\n",
      "Epoch 242/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3555 - val_loss: 0.2686 - lr: 0.0010\n",
      "Epoch 243/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3570 - val_loss: 0.2704 - lr: 0.0010\n",
      "Epoch 244/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.2714 - lr: 0.0010\n",
      "Epoch 245/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3472 - val_loss: 0.2691 - lr: 0.0010\n",
      "Epoch 246/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3558 - val_loss: 0.2673 - lr: 0.0010\n",
      "Epoch 247/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3625 - val_loss: 0.2680 - lr: 0.0010\n",
      "Epoch 248/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3773 - val_loss: 0.2678 - lr: 0.0010\n",
      "Epoch 249/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3381 - val_loss: 0.2660 - lr: 0.0010\n",
      "Epoch 250/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.2685 - lr: 0.0010\n",
      "Epoch 251/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.2691 - lr: 0.0010\n",
      "Epoch 252/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3759 - val_loss: 0.2672 - lr: 0.0010\n",
      "Epoch 253/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3826 - val_loss: 0.2640 - lr: 0.0010\n",
      "Epoch 254/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3654 - val_loss: 0.2604 - lr: 0.0010\n",
      "Epoch 255/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.2585 - lr: 0.0010\n",
      "Epoch 256/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.2590 - lr: 0.0010\n",
      "Epoch 257/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3642 - val_loss: 0.2615 - lr: 0.0010\n",
      "Epoch 258/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3857 - val_loss: 0.2618 - lr: 0.0010\n",
      "Epoch 259/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3706 - val_loss: 0.2622 - lr: 0.0010\n",
      "Epoch 260/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3670 - val_loss: 0.2615 - lr: 0.0010\n",
      "Epoch 261/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3371 - val_loss: 0.2612 - lr: 0.0010\n",
      "Epoch 262/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3586 - val_loss: 0.2596 - lr: 0.0010\n",
      "Epoch 263/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3598 - val_loss: 0.2604 - lr: 0.0010\n",
      "Epoch 264/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3324 - val_loss: 0.2652 - lr: 0.0010\n",
      "Epoch 265/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.2678 - lr: 0.0010\n",
      "Epoch 266/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3421 - val_loss: 0.2671 - lr: 0.0010\n",
      "Epoch 267/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3463 - val_loss: 0.2667 - lr: 0.0010\n",
      "Epoch 268/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 0.2652 - lr: 0.0010\n",
      "Epoch 269/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3675 - val_loss: 0.2638 - lr: 0.0010\n",
      "Epoch 270/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3422 - val_loss: 0.2588 - lr: 0.0010\n",
      "Epoch 271/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3485 - val_loss: 0.2539 - lr: 0.0010\n",
      "Epoch 272/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.2515 - lr: 0.0010\n",
      "Epoch 273/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.2534 - lr: 0.0010\n",
      "Epoch 274/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3458 - val_loss: 0.2554 - lr: 0.0010\n",
      "Epoch 275/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 0.2546 - lr: 0.0010\n",
      "Epoch 276/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3659 - val_loss: 0.2552 - lr: 0.0010\n",
      "Epoch 277/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3556 - val_loss: 0.2552 - lr: 0.0010\n",
      "Epoch 278/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3578 - val_loss: 0.2551 - lr: 0.0010\n",
      "Epoch 279/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.2564 - lr: 0.0010\n",
      "Epoch 280/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3331 - val_loss: 0.2589 - lr: 0.0010\n",
      "Epoch 281/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3252 - val_loss: 0.2598 - lr: 0.0010\n",
      "Epoch 282/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 0.2651 - lr: 0.0010\n",
      "Epoch 283/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3401 - val_loss: 0.2662 - lr: 0.0010\n",
      "Epoch 284/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.2622 - lr: 0.0010\n",
      "Epoch 285/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3568 - val_loss: 0.2562 - lr: 0.0010\n",
      "Epoch 286/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3444 - val_loss: 0.2540 - lr: 0.0010\n",
      "Epoch 287/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3626 - val_loss: 0.2539 - lr: 0.0010\n",
      "Epoch 288/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.2555 - lr: 0.0010\n",
      "Epoch 289/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3566 - val_loss: 0.2542 - lr: 0.0010\n",
      "Epoch 290/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3314 - val_loss: 0.2523 - lr: 0.0010\n",
      "Epoch 291/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3515 - val_loss: 0.2532 - lr: 0.0010\n",
      "Epoch 292/2000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3457 - val_loss: 0.2519 - lr: 0.0010\n",
      "Epoch 292: early stopping\n",
      "16/16 [==============================] - 0s 823us/step\n",
      "0.896\n"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=20)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=5, min_lr=0.001)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=20, activation=\"selu\", dropout=0.3, metrics=['accuracy'], batch_size=64, epochs=2000, callbacks=[es, reduce_lr], validation_split=0.1, verbose=1)  # for notebook execution time\n",
    "\n",
    "# check score\n",
    "clf.fit(X, y)\n",
    "print(clf.score(X, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04f8d5e9-20b2-40c1-b7cf-ba2bd2df4a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=5, noise=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a04244e-be48-4e13-9a1f-d25694578ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "class MLPRegressor(KerasRegressor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=100,\n",
    "        activation=\"relu\",\n",
    "        batch_normalization=True,\n",
    "        dropout=0,\n",
    "        optimizer=\"adam\",\n",
    "        optimizer__learning_rate=0.001,\n",
    "        epochs=200,\n",
    "        verbose=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout = dropout\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs: Dict[str, Any]):\n",
    "        model = keras.Sequential()\n",
    "        inp = keras.layers.Input(shape=(self.n_features_in_))\n",
    "        model.add(inp)\n",
    "        for hidden_layer_size in (self.hidden_layer_sizes,):\n",
    "            layer = keras.layers.Dense(hidden_layer_size, activation=self.activation)\n",
    "            model.add(layer)\n",
    "            if self.batch_normalization:\n",
    "               layer = keras.layers.BatchNormalization()\n",
    "               model.add(layer)\n",
    "            if self.dropout > 0:\n",
    "               layer = keras.layers.Dropout(self.dropout)\n",
    "               model.add(layer)    \n",
    "        out = keras.layers.Dense(1)\n",
    "        model.add(out)\n",
    "        model.compile(loss=\"mse\", optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93e23542-c470-4d14-8e1c-78ea02e84bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13606.5547 - val_loss: 8948.4385 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13506.3857 - val_loss: 8918.2266 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13445.3301 - val_loss: 8886.7402 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13341.6611 - val_loss: 8853.5820 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13245.8203 - val_loss: 8818.0771 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13170.6963 - val_loss: 8779.9609 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13114.1641 - val_loss: 8740.5811 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13016.8809 - val_loss: 8699.1289 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12872.5967 - val_loss: 8655.4961 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12813.5127 - val_loss: 8607.9023 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12717.5303 - val_loss: 8557.8047 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12607.3730 - val_loss: 8505.8203 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12507.9805 - val_loss: 8451.0898 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12417.8662 - val_loss: 8393.8027 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12363.4297 - val_loss: 8332.3125 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12191.5781 - val_loss: 8268.9482 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12162.8203 - val_loss: 8202.1396 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12038.1943 - val_loss: 8137.1973 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11878.0098 - val_loss: 8067.4512 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11810.5107 - val_loss: 7996.0698 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11782.7529 - val_loss: 7923.4951 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11649.2373 - val_loss: 7845.3579 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11457.8086 - val_loss: 7763.2588 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11386.8311 - val_loss: 7679.1660 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11270.6641 - val_loss: 7597.1519 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11118.1523 - val_loss: 7506.2368 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11043.7549 - val_loss: 7416.3643 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10958.2637 - val_loss: 7331.5830 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10831.4131 - val_loss: 7233.0918 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10715.5342 - val_loss: 7138.7471 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10499.5664 - val_loss: 7043.0288 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10421.8438 - val_loss: 6944.8374 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10382.6865 - val_loss: 6840.2627 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10123.2363 - val_loss: 6739.2612 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10013.1836 - val_loss: 6632.6890 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9861.3936 - val_loss: 6525.0068 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9680.9707 - val_loss: 6420.2524 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9518.2578 - val_loss: 6306.3892 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9391.6406 - val_loss: 6197.1680 - lr: 0.0010\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9272.6768 - val_loss: 6078.2905 - lr: 0.0010\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9112.9336 - val_loss: 5963.3687 - lr: 0.0010\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8870.4102 - val_loss: 5846.3726 - lr: 0.0010\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8951.7871 - val_loss: 5731.1489 - lr: 0.0010\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8729.4990 - val_loss: 5620.9839 - lr: 0.0010\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8680.2412 - val_loss: 5509.3013 - lr: 0.0010\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8328.6211 - val_loss: 5388.9985 - lr: 0.0010\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8379.9922 - val_loss: 5269.1904 - lr: 0.0010\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8069.2744 - val_loss: 5154.9297 - lr: 0.0010\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7923.6689 - val_loss: 5050.2676 - lr: 0.0010\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7842.6528 - val_loss: 4929.2852 - lr: 0.0010\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7714.5151 - val_loss: 4804.3398 - lr: 0.0010\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7667.4111 - val_loss: 4689.7607 - lr: 0.0010\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7371.0654 - val_loss: 4585.2471 - lr: 0.0010\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7098.5864 - val_loss: 4478.3042 - lr: 0.0010\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7143.0151 - val_loss: 4365.6558 - lr: 0.0010\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6949.4512 - val_loss: 4258.3530 - lr: 0.0010\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6812.6050 - val_loss: 4142.2969 - lr: 0.0010\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6665.3228 - val_loss: 4034.8643 - lr: 0.0010\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6377.6211 - val_loss: 3952.2556 - lr: 0.0010\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6311.3013 - val_loss: 3840.2017 - lr: 0.0010\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6068.0054 - val_loss: 3731.9041 - lr: 0.0010\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6169.8589 - val_loss: 3622.0796 - lr: 0.0010\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5999.6826 - val_loss: 3540.2451 - lr: 0.0010\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5638.6670 - val_loss: 3461.9634 - lr: 0.0010\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5805.5469 - val_loss: 3354.8569 - lr: 0.0010\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5613.9233 - val_loss: 3253.9360 - lr: 0.0010\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5595.9087 - val_loss: 3145.9128 - lr: 0.0010\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5185.7974 - val_loss: 3059.1135 - lr: 0.0010\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5172.5044 - val_loss: 2967.7063 - lr: 0.0010\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5079.3472 - val_loss: 2897.0076 - lr: 0.0010\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4790.2954 - val_loss: 2791.2947 - lr: 0.0010\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4751.9404 - val_loss: 2696.7847 - lr: 0.0010\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4753.0312 - val_loss: 2612.7705 - lr: 0.0010\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4453.7495 - val_loss: 2518.8235 - lr: 0.0010\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4286.3193 - val_loss: 2430.1858 - lr: 0.0010\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4226.7744 - val_loss: 2387.2627 - lr: 0.0010\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4236.2773 - val_loss: 2293.3140 - lr: 0.0010\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4078.8123 - val_loss: 2211.9209 - lr: 0.0010\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3905.0930 - val_loss: 2149.5881 - lr: 0.0010\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3706.8799 - val_loss: 2064.7432 - lr: 0.0010\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3799.1040 - val_loss: 2025.1936 - lr: 0.0010\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3502.1375 - val_loss: 1939.6672 - lr: 0.0010\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3521.3870 - val_loss: 1866.0864 - lr: 0.0010\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3511.2104 - val_loss: 1788.2710 - lr: 0.0010\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3290.8477 - val_loss: 1716.8069 - lr: 0.0010\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3299.6443 - val_loss: 1648.2690 - lr: 0.0010\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3356.3970 - val_loss: 1601.8953 - lr: 0.0010\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2880.5134 - val_loss: 1534.9769 - lr: 0.0010\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3100.1755 - val_loss: 1517.7450 - lr: 0.0010\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2987.9041 - val_loss: 1442.6173 - lr: 0.0010\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2932.0208 - val_loss: 1372.7439 - lr: 0.0010\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2956.4976 - val_loss: 1335.6614 - lr: 0.0010\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2665.3662 - val_loss: 1268.6161 - lr: 0.0010\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2509.9556 - val_loss: 1241.6105 - lr: 0.0010\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2705.2986 - val_loss: 1177.3014 - lr: 0.0010\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2283.8101 - val_loss: 1129.8124 - lr: 0.0010\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2408.2493 - val_loss: 1095.3325 - lr: 0.0010\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2445.8391 - val_loss: 1055.7347 - lr: 0.0010\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2256.6672 - val_loss: 1025.5046 - lr: 0.0010\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2206.3521 - val_loss: 984.3870 - lr: 0.0010\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2187.8000 - val_loss: 922.4674 - lr: 0.0010\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2214.0688 - val_loss: 866.4943 - lr: 0.0010\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2166.8826 - val_loss: 830.7693 - lr: 0.0010\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2110.5266 - val_loss: 791.8489 - lr: 0.0010\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1934.3815 - val_loss: 751.3566 - lr: 0.0010\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1953.5193 - val_loss: 709.9556 - lr: 0.0010\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1761.1210 - val_loss: 695.0164 - lr: 0.0010\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1758.9736 - val_loss: 662.1041 - lr: 0.0010\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1660.0120 - val_loss: 643.7184 - lr: 0.0010\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1681.6641 - val_loss: 601.3653 - lr: 0.0010\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1396.9010 - val_loss: 578.4661 - lr: 0.0010\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1649.7676 - val_loss: 544.0132 - lr: 0.0010\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1722.0024 - val_loss: 524.0261 - lr: 0.0010\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1276.2961 - val_loss: 489.3918 - lr: 0.0010\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1501.7057 - val_loss: 471.2874 - lr: 0.0010\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1408.4727 - val_loss: 442.1577 - lr: 0.0010\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1411.0359 - val_loss: 420.0841 - lr: 0.0010\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1363.7458 - val_loss: 401.2504 - lr: 0.0010\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1309.9749 - val_loss: 377.5178 - lr: 0.0010\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1164.2223 - val_loss: 359.2062 - lr: 0.0010\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1209.0786 - val_loss: 343.4956 - lr: 0.0010\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1324.4176 - val_loss: 323.0310 - lr: 0.0010\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1429.6488 - val_loss: 355.4717 - lr: 0.0010\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1273.7605 - val_loss: 340.2435 - lr: 0.0010\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1172.5785 - val_loss: 313.0595 - lr: 0.0010\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1104.8923 - val_loss: 296.4821 - lr: 0.0010\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1241.2765 - val_loss: 297.2056 - lr: 0.0010\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 956.3832 - val_loss: 317.2274 - lr: 0.0010\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 873.8059 - val_loss: 301.4907 - lr: 0.0010\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1142.6655 - val_loss: 277.7133 - lr: 0.0010\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 921.0746 - val_loss: 255.1828 - lr: 0.0010\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1120.8356 - val_loss: 243.9257 - lr: 0.0010\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 835.4078 - val_loss: 232.8211 - lr: 0.0010\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 801.3705 - val_loss: 224.1431 - lr: 0.0010\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1069.6420 - val_loss: 200.6792 - lr: 0.0010\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 977.0823 - val_loss: 186.2216 - lr: 0.0010\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 791.4527 - val_loss: 179.6850 - lr: 0.0010\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 939.9983 - val_loss: 186.6427 - lr: 0.0010\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 802.3350 - val_loss: 176.9619 - lr: 0.0010\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 729.1793 - val_loss: 164.5931 - lr: 0.0010\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 849.2254 - val_loss: 159.9742 - lr: 0.0010\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 768.1040 - val_loss: 147.0445 - lr: 0.0010\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 746.0126 - val_loss: 151.0326 - lr: 0.0010\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 820.7451 - val_loss: 148.9988 - lr: 0.0010\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 707.5533 - val_loss: 140.1774 - lr: 0.0010\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 867.3251 - val_loss: 131.9986 - lr: 0.0010\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 770.4237 - val_loss: 119.5650 - lr: 0.0010\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.0410 - val_loss: 121.1913 - lr: 0.0010\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 668.3838 - val_loss: 111.5303 - lr: 0.0010\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 564.1797 - val_loss: 104.3899 - lr: 0.0010\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 807.5067 - val_loss: 92.0211 - lr: 0.0010\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 689.4572 - val_loss: 84.5575 - lr: 0.0010\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 668.3369 - val_loss: 81.4319 - lr: 0.0010\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 708.6694 - val_loss: 81.2710 - lr: 0.0010\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 830.8411 - val_loss: 73.2827 - lr: 0.0010\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 765.6616 - val_loss: 77.3182 - lr: 0.0010\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 575.8436 - val_loss: 73.7085 - lr: 0.0010\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 580.7220 - val_loss: 69.6505 - lr: 0.0010\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 821.0924 - val_loss: 63.1132 - lr: 0.0010\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.9509 - val_loss: 61.3827 - lr: 0.0010\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 660.5629 - val_loss: 59.9194 - lr: 0.0010\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 702.4508 - val_loss: 67.4529 - lr: 0.0010\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 829.4750 - val_loss: 64.7923 - lr: 0.0010\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 700.0153 - val_loss: 60.4984 - lr: 0.0010\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 662.8310 - val_loss: 54.7477 - lr: 0.0010\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 758.8398 - val_loss: 50.7477 - lr: 0.0010\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 894.9810 - val_loss: 48.5079 - lr: 0.0010\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 650.2599 - val_loss: 45.9013 - lr: 0.0010\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 801.0475 - val_loss: 42.8633 - lr: 0.0010\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 801.3552 - val_loss: 39.4772 - lr: 0.0010\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 736.6895 - val_loss: 37.9525 - lr: 0.0010\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 778.4954 - val_loss: 35.4701 - lr: 0.0010\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 927.2236 - val_loss: 33.3866 - lr: 0.0010\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 653.5167 - val_loss: 39.5211 - lr: 0.0010\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 603.9966 - val_loss: 37.5513 - lr: 0.0010\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 483.3699 - val_loss: 38.5202 - lr: 0.0010\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.3917 - val_loss: 44.8446 - lr: 0.0010\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 450.9237 - val_loss: 42.9497 - lr: 0.0010\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 696.6622 - val_loss: 41.9736 - lr: 0.0010\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 784.1921 - val_loss: 38.6423 - lr: 0.0010\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 603.7372 - val_loss: 34.9607 - lr: 0.0010\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 812.3227 - val_loss: 32.5100 - lr: 0.0010\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 577.9482 - val_loss: 28.9806 - lr: 0.0010\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 468.4357 - val_loss: 26.9280 - lr: 0.0010\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 540.1650 - val_loss: 25.0684 - lr: 0.0010\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 725.8937 - val_loss: 21.7909 - lr: 0.0010\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 792.5956 - val_loss: 22.9904 - lr: 0.0010\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 717.4424 - val_loss: 25.7732 - lr: 0.0010\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.8214 - val_loss: 26.4898 - lr: 0.0010\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 646.4626 - val_loss: 23.5084 - lr: 0.0010\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 789.9594 - val_loss: 21.3149 - lr: 0.0010\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 871.2654 - val_loss: 21.1588 - lr: 0.0010\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 776.2849 - val_loss: 17.2447 - lr: 0.0010\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 688.5779 - val_loss: 14.7626 - lr: 0.0010\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 586.8872 - val_loss: 15.0902 - lr: 0.0010\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.4874 - val_loss: 16.6737 - lr: 0.0010\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 761.1250 - val_loss: 13.3814 - lr: 0.0010\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 520.6788 - val_loss: 11.6114 - lr: 0.0010\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 678.6783 - val_loss: 11.4180 - lr: 0.0010\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 699.2211 - val_loss: 14.1573 - lr: 0.0010\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 447.0440 - val_loss: 21.0515 - lr: 0.0010\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 820.1326 - val_loss: 25.7644 - lr: 0.0010\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 668.1665 - val_loss: 29.2964 - lr: 0.0010\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 781.2365 - val_loss: 21.5022 - lr: 0.0010\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 593.7723 - val_loss: 19.0596 - lr: 0.0010\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 671.1247 - val_loss: 17.9789 - lr: 0.0010\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 680.0261 - val_loss: 16.8931 - lr: 0.0010\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 578.2257 - val_loss: 16.0552 - lr: 0.0010\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.2561 - val_loss: 16.5318 - lr: 0.0010\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 798.5484 - val_loss: 16.4913 - lr: 0.0010\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 519.3373 - val_loss: 17.4039 - lr: 0.0010\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 541.0646 - val_loss: 17.4810 - lr: 0.0010\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 921.2383 - val_loss: 20.8881 - lr: 0.0010\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 576.4180 - val_loss: 25.3359 - lr: 0.0010\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 603.2411 - val_loss: 25.8271 - lr: 0.0010\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 602.4642 - val_loss: 21.5756 - lr: 0.0010\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 582.5876 - val_loss: 19.3100 - lr: 0.0010\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 588.4753 - val_loss: 22.0801 - lr: 0.0010\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 649.7947 - val_loss: 29.1683 - lr: 0.0010\n",
      "Epoch 219: early stopping\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "0.9962858237061316\n"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=20)\n",
    "reduce_lr =keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=5, min_lr=0.001)\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes=50, activation=\"selu\", batch_normalization=True, dropout=0.5, batch_size=64, epochs=2000, callbacks=[es, reduce_lr], validation_split=0.1, verbose=1)  # for notebook execution time\n",
    "\n",
    "# check score\n",
    "print(reg.fit(X, y).score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5046aa-b027-46a5-a906-0092acaab2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezstacking",
   "language": "python",
   "name": "ezstacking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
